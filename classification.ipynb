{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85241b9",
   "metadata": {},
   "source": [
    "# From CNNs to ViTs: A Comparative Study in Medical Image Classification\n",
    "\n",
    "Image classification is a fundamental task in computer vision that involves assigning a label or category to an input image. This project explores two prominent deep learning architectures for medical image classification:\n",
    "\n",
    "- **Convolutional Neural Networks (CNNs)**: Traditional deep learning models that use convolutional operations to extract hierarchical features from images\n",
    "- **Vision Transformers (ViTs)**: Modern architecture that adapts the Transformer model (originally designed for NLP) to process images as sequences of patches\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Image Classification**: The process of automatically categorizing images into predefined classes based on their visual content\n",
    "- **Medical Imaging**: Application of classification to medical scans (CT scans, X-rays, MRIs) for diagnostic assistance\n",
    "- **Comparative Study**: This project compares the effectiveness of CNNs and ViTs on the Medical MNIST dataset to understand their relative strengths and weaknesses\n",
    "\n",
    "### References\n",
    "\n",
    "- **Convolutional Neural Networks**: LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*, 86(11), 2278-2324.\n",
    "- **Vision Transformers**: Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.\n",
    "- **Hugging Face**: Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., ... & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. *Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations*, 38-45.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f1e1b80e50168",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Set up running environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a628ed34b4e3467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:09.494937Z",
     "start_time": "2025-11-17T11:26:06.892665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.9/site-packages (25.3)\n",
      "Requirement already satisfied: kagglehub in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.3.13)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.23.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.57.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (11.3.0)\n",
      "Requirement already satisfied: ultralytics in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (8.3.225)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (6.0.3)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (2.3.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (3.9.4)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from kagglehub->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from kagglehub->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from kagglehub->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 2)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from torchvision->-r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 4)) (0.36.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 4)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 4)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 4)) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.9/site-packages (from accelerate>=0.26.0->-r requirements.txt (line 5)) (7.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 8)) (4.12.0.88)\n",
      "Requirement already satisfied: polars in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 8)) (1.35.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in ./.venv/lib/python3.9/site-packages (from ultralytics->-r requirements.txt (line 8)) (2.0.18)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 10)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 10)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 10)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 11)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 11)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 11)) (3.2.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 11)) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 11)) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->kagglehub->-r requirements.txt (line 1)) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.3)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.1 in ./.venv/lib/python3.9/site-packages (from polars->ultralytics->-r requirements.txt (line 8)) (1.35.1)\n",
      "✓ Virtual environment created and activated\n",
      "✓ All dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# ============================================================================\n",
    "# Environment Setup: Creating Virtual Environment and Installing Dependencies\n",
    "# ============================================================================\n",
    "# This cell sets up a clean Python environment for the project by:\n",
    "# 1. Creating an isolated virtual environment to avoid dependency conflicts\n",
    "# 2. Activating the virtual environment for this session\n",
    "# 3. Installing all required packages from requirements.txt\n",
    "\n",
    "# Step 1: Create a new virtual environment named '.venv'\n",
    "# This isolates project dependencies from the system Python installation\n",
    "python3 -m venv .venv\n",
    "\n",
    "# Step 2: Activate the virtual environment\n",
    "# This ensures that all subsequent Python commands use packages from .venv\n",
    "source .venv/bin/activate\n",
    "\n",
    "# Step 3: Upgrade pip to the latest version for better package management\n",
    "pip install --upgrade pip\n",
    "\n",
    "# Step 4: Install all project dependencies from requirements.txt\n",
    "# This installs all necessary libraries for CNN and ViT model training\n",
    "pip install -r requirements.txt\n",
    "\n",
    "echo \"✓ Virtual environment created and activated\"\n",
    "echo \"✓ All dependencies installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e99da",
   "metadata": {},
   "source": [
    "## Dataset: Medical MNIST\n",
    "\n",
    "The **Medical MNIST** dataset is a collection of medical imaging data that will be downloaded from Kaggle. This dataset serves as an excellent benchmark for comparing the performance of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in medical image classification tasks.\n",
    "\n",
    "### Dataset Overview\n",
    "- **Source**: Kaggle\n",
    "- **Total Classes**: 6\n",
    "- **Image Type**: Medical imaging scans (CT scans, X-rays, MRI)\n",
    "- **Purpose**: Multi-class classification of different medical imaging modalities\n",
    "\n",
    "### Class Definitions\n",
    "\n",
    "The dataset contains **6 distinct classes**, each representing a different type of medical imaging:\n",
    "\n",
    "1. **AbdomenCT** - Abdominal Computed Tomography (CT) scans\n",
    "   - Images of the abdominal region captured using CT imaging technology\n",
    "   - Used for diagnosing conditions in organs like liver, kidneys, and intestines\n",
    "\n",
    "2. **BreastMRI** - Breast Magnetic Resonance Imaging (MRI) scans\n",
    "   - High-resolution MRI images of breast tissue\n",
    "   - Commonly used for breast cancer detection and diagnosis\n",
    "\n",
    "3. **ChestCT** - Chest Computed Tomography (CT) scans\n",
    "   - CT images of the thoracic region (chest area)\n",
    "   - Used for detecting lung diseases, tumors, and other chest abnormalities\n",
    "\n",
    "4. **ChestXray** - Chest X-ray images\n",
    "   - Traditional X-ray images of the chest\n",
    "   - One of the most common medical imaging techniques for lung and heart assessment\n",
    "\n",
    "5. **Hand** - Hand X-ray images\n",
    "   - X-ray images of the hand and wrist\n",
    "   - Used for diagnosing fractures, arthritis, and other bone-related conditions\n",
    "\n",
    "6. **HeadCT** - Head Computed Tomography (CT) scans\n",
    "   - CT scans of the head and brain region\n",
    "   - Critical for diagnosing brain injuries, tumors, and neurological conditions\n",
    "\n",
    "### Dataset Characteristics\n",
    "- Each class contains medical images that require careful analysis and classification\n",
    "- The dataset presents a challenging classification problem due to the visual similarities between some medical imaging modalities\n",
    "- This diversity makes it an ideal testbed for evaluating the effectiveness of different deep learning architectures in medical imaging applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbd4cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:16.343173Z",
     "start_time": "2025-11-17T11:26:16.284916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28497c0589af4417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:22.054868Z",
     "start_time": "2025-11-17T11:26:22.051173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data directory already exists: /Users/sztaki/Documents/computer_vision/data\n"
     ]
    }
   ],
   "source": [
    "# Define the data directory in the current working directory\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Check if data directory exists, create if not\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ Created data directory: {data_dir.absolute()}\")\n",
    "else:\n",
    "    print(f\"✓ Data directory already exists: {data_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12701d36bcb1ca3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:23.906033Z",
     "start_time": "2025-11-17T11:26:23.871336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset already exists in /Users/sztaki/Documents/computer_vision/data\n",
      "  Path: /Users/sztaki/Documents/computer_vision/data/medical-mnist-train-test-val\n"
     ]
    }
   ],
   "source": [
    "# Check if dataset is already in the data directory\n",
    "dataset_in_data = data_dir / \"medical-mnist-train-test-val\"\n",
    "train_dir = dataset_in_data / \"train\"\n",
    "if dataset_in_data.exists() and train_dir.exists() and any(train_dir.iterdir()):\n",
    "    print(f\"✓ Dataset already exists in {data_dir.absolute()}\")\n",
    "    print(f\"  Path: {dataset_in_data.absolute()}\")\n",
    "else:\n",
    "    # Download latest version to cache\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    cache_path = kagglehub.dataset_download(\"gennadiimanzhos/medical-mnist-train-test-val\")\n",
    "    print(f\"✓ Downloaded to cache: {cache_path}\")\n",
    "\n",
    "    # Copy dataset from cache to data directory\n",
    "    print(f\"Copying dataset to {data_dir.absolute()}...\")\n",
    "    if dataset_in_data.exists():\n",
    "        shutil.rmtree(dataset_in_data)\n",
    "    shutil.copytree(cache_path, dataset_in_data)\n",
    "    print(f\"✓ Dataset copied to: {dataset_in_data.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9913cd85ebbe88c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:26.277516Z",
     "start_time": "2025-11-17T11:26:26.274431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset ready at: /Users/sztaki/Documents/computer_vision/data/medical-mnist-train-test-val\n"
     ]
    }
   ],
   "source": [
    "# Set the final path for use in the notebook\n",
    "dataset_path = dataset_in_data\n",
    "print(f\"\\n✓ Dataset ready at: {dataset_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436b789",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This section covers the data loading process for the Medical MNIST dataset. The dataset comes pre-split into three distinct sets: **training**, **validation**, and **test** sets. Each set contains images organized by class in separate directories.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The Medical MNIST dataset is organized as follows:\n",
    "- **Train Set**: Used for model training and learning patterns\n",
    "- **Validation Set**: Used for hyperparameter tuning and model selection during training\n",
    "- **Test Set**: Used for final evaluation of model performance\n",
    "\n",
    "Each split contains 6 class directories:\n",
    "- `AbdomenCT/`\n",
    "- `BreastMRI/`\n",
    "- `ChestCT/`\n",
    "- `CXR/` (Chest X-ray)\n",
    "- `Hand/`\n",
    "- `HeadCT/`\n",
    "\n",
    "### Data Loading Process\n",
    "\n",
    "The data loading pipeline will:\n",
    "\n",
    "1. **Load Training Data**: Load images from the `train/` directory with their corresponding class labels\n",
    "2. **Load Validation Data**: Load images from the `val/` directory for validation during training\n",
    "3. **Load Test Data**: Load images from the `test/` directory for final model evaluation\n",
    "4. **Train-Test Split**: The dataset already provides separate train, validation, and test splits, so we will use these predefined splits directly\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "- Images will be loaded and preprocessed (resizing, normalization, etc.)\n",
    "- Data loaders will be created with appropriate batch sizes for efficient training\n",
    "- Data augmentation may be applied to the training set to improve model generalization\n",
    "- Class labels will be encoded appropriately for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff28f9bbbf5d42aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:31.024950Z",
     "start_time": "2025-11-17T11:26:30.057598Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c3d5fc1c78ee97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:32.119005Z",
     "start_time": "2025-11-17T11:26:32.095776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MPS (Apple Silicon GPU) detected and will be used\n",
      "  This will use the GPU cores on your Apple Silicon chip\n",
      "\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device - check for GPU/accelerator availability\n",
    "# Priority: CUDA (NVIDIA GPU) > MPS (Apple Silicon GPU) > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✓ CUDA (NVIDIA GPU) detected and will be used\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"✓ MPS (Apple Silicon GPU) detected and will be used\")\n",
    "    print(f\"  This will use the GPU cores on your Apple Silicon chip\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"⚠ No GPU acceleration available, using CPU\")\n",
    "    print(f\"  Note: Training will be slower on CPU\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed5eee3392b50b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:35.424257Z",
     "start_time": "2025-11-17T11:26:35.420853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_dir = dataset_path / \"train\"\n",
    "val_dir = dataset_path / \"val\"\n",
    "test_dir = dataset_path / \"test\"\n",
    "\n",
    "# ResNet expects ImageNet-style normalization\n",
    "# Mean and std for ImageNet (standard for ResNet)\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Training transforms (no augmentation - dataset is large enough)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# No augmentation for validation and test sets\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30ab4efa14bb755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:38.264452Z",
     "start_time": "2025-11-17T11:26:38.107989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=str(train_dir), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(root=str(val_dir), transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=str(test_dir), transform=val_test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b1845224811e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:39.686576Z",
     "start_time": "2025-11-17T11:26:39.682457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classes: 6\n",
      "Class names: ['AbdomenCT', 'BreastMRI', 'CXR', 'ChestCT', 'Hand', 'HeadCT']\n"
     ]
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "num_workers = 4 if os.cpu_count() > 4 else 2\n",
    "\n",
    "# pin_memory only works with CUDA, not MPS\n",
    "use_pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=use_pin_memory\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=use_pin_memory\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=use_pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5c48eb5da6ae37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:26:56.243919Z",
     "start_time": "2025-11-17T11:26:42.465565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "  Training samples: 47163\n",
      "  Validation samples: 5895\n",
      "  Test samples: 5896\n",
      "  Total samples: 58954\n",
      "\n",
      "DataLoader Configuration:\n",
      "  Batch size: 32\n",
      "  Number of workers: 4\n",
      "  Number of batches (train): 1474\n",
      "  Number of batches (val): 185\n",
      "  Number of batches (test): 185\n",
      "\n",
      "Sample batch shape:\n",
      "  Images shape: torch.Size([32, 3, 224, 224])\n",
      "  Labels shape: torch.Size([32])\n",
      "  Image dtype: torch.float32\n",
      "  Label dtype: torch.int64\n",
      "\n",
      "✓ Data loaders created successfully and ready for NN training!\n"
     ]
    }
   ],
   "source": [
    "# Print dataset statistics\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Total samples: {len(train_dataset) + len(val_dataset) + len(test_dataset)}\")\n",
    "print(f\"\\nDataLoader Configuration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Number of workers: {num_workers}\")\n",
    "print(f\"  Number of batches (train): {len(train_loader)}\")\n",
    "print(f\"  Number of batches (val): {len(val_loader)}\")\n",
    "print(f\"  Number of batches (test): {len(test_loader)}\")\n",
    "\n",
    "# Verify data loading by getting a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "images, labels = sample_batch\n",
    "print(f\"\\nSample batch shape:\")\n",
    "print(f\"  Images shape: {images.shape}\")  # [batch_size, channels, height, width]\n",
    "print(f\"  Labels shape: {labels.shape}\")  # [batch_size]\n",
    "print(f\"  Image dtype: {images.dtype}\")\n",
    "print(f\"  Label dtype: {labels.dtype}\")\n",
    "\n",
    "print(\"\\n✓ Data loaders created successfully and ready for NN training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8a706",
   "metadata": {},
   "source": [
    "## Custom Small Neural Network\n",
    "\n",
    "After training with the pre-trained ResNet-50 architecture, we will now train a **custom lightweight neural network** from scratch. This provides an interesting comparison between:\n",
    "\n",
    "- **Transfer Learning (ResNet-50)**: Using a large, pre-trained model with millions of parameters\n",
    "- **Custom Small Network**: Training a compact, task-specific architecture from scratch\n",
    "\n",
    "### Architecture Design\n",
    "\n",
    "The custom neural network will be a simple but effective **Convolutional Neural Network (CNN)** designed specifically for the Medical MNIST classification task:\n",
    "\n",
    "**Network Structure:**\n",
    "- **Input Layer**: 224×224×3 RGB images\n",
    "- **Convolutional Blocks**: Multiple conv layers with increasing depth\n",
    "  - Convolutional layers with ReLU activation\n",
    "  - Max pooling for dimensionality reduction\n",
    "  - Batch normalization for training stability\n",
    "- **Fully Connected Layers**: Dense layers for classification\n",
    "- **Output Layer**: 6 classes (one for each medical imaging modality)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Lightweight**: Significantly fewer parameters than ResNet-50\n",
    "- **Task-Specific**: Designed from scratch for medical image classification\n",
    "- **Fast Training**: Smaller model trains faster and requires less memory\n",
    "- **No Pre-training**: Trained from random initialization (no transfer learning)\n",
    "\n",
    "### Training Approach\n",
    "\n",
    "The custom network will be trained using the same dataset and evaluation methodology as ResNet-50, allowing for a fair comparison of:\n",
    "- **Model Size**: Number of parameters\n",
    "- **Training Time**: Time to convergence\n",
    "- **Performance**: Accuracy on validation and test sets\n",
    "- **Efficiency**: Model size vs. performance trade-off\n",
    "\n",
    "This comparison will help understand whether the complexity of a large pre-trained model is necessary, or if a simpler custom architecture can achieve comparable results for this specific medical imaging classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5008be7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:27:00.192155Z",
     "start_time": "2025-11-17T11:27:00.189780Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# ============================================================================\n",
    "# Custom CNN Model Definition\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b080c0a42d05b37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:27:01.581253Z",
     "start_time": "2025-11-17T11:27:01.529777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Custom CNN model...\n",
      "✓ Model initialized and moved to mps\n",
      "  Total parameters: 3,438,342\n",
      "  Trainable parameters: 3,438,342\n"
     ]
    }
   ],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 224x224 -> 112x112\n",
    "        )\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 112x112 -> 56x56\n",
    "        )\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 56x56 -> 28x28\n",
    "        )\n",
    "\n",
    "        # Additional pooling to reduce size further\n",
    "        self.pool = nn.AdaptiveAvgPool2d((7, 7))  # 28x28 -> 7x7\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# Model Initialization\n",
    "# ============================================================================\n",
    "print(\"Initializing Custom CNN model...\")\n",
    "\n",
    "# Create model instance\n",
    "custom_model = CustomCNN(num_classes=num_classes)\n",
    "\n",
    "# Move model to device\n",
    "custom_model = custom_model.to(device)\n",
    "print(f\"✓ Model initialized and moved to {device}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in custom_model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in custom_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edb5d22698a08d24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:27:08.672273Z",
     "start_time": "2025-11-17T11:27:08.664989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration:\n",
      "  Epochs: 20\n",
      "  Learning rate: 0.001\n",
      "  Optimizer: Adam\n",
      "  Loss function: CrossEntropyLoss\n",
      "  Early stopping patience: 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Training Configuration\n",
    "# ============================================================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(custom_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = copy.deepcopy(custom_model.state_dict())\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Training history\n",
    "history_custom = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")\n",
    "print(f\"  Early stopping patience: {patience}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8132c8a0931ca8a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:27:13.059403Z",
     "start_time": "2025-11-17T11:27:13.054082Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Training and Validation Functions\n",
    "# ============================================================================\n",
    "def train_epoch_custom(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        current_batch = total // labels.size(0)\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{running_loss/current_batch:.4f}',\n",
    "            'acc': f'{100*correct/total:.2f}%'\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d755db3198c854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:29:58.861509Z",
     "start_time": "2025-11-17T11:27:19.261123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Custom CNN Training...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/20\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1474/1474 [01:56<00:00, 12.62it/s, loss=0.0563, acc=98.07%]\n",
      "Validation: 100%|██████████| 185/185 [00:27<00:00,  6.82it/s, loss=0.0032, acc=99.69%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Results:\n",
      "  Train Loss: 0.0667 | Train Acc: 98.07%\n",
      "  Val Loss: 0.0144 | Val Acc: 99.69%\n",
      "  Learning Rate: 0.001000\n",
      "  ✓ New best validation loss: 0.0144\n",
      "\n",
      "Epoch 2/20\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 139/1474 [00:12<01:23, 15.99it/s, loss=0.0157, acc=99.57%] Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10b16aa60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "Training:   9%|▉         | 139/1474 [00:15<02:29,  8.92it/s, loss=0.0157, acc=99.57%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_custom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[1;32m     47\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate_epoch_custom(custom_model, val_loader, criterion, device)\n",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m, in \u001b[0;36mtrain_epoch_custom\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def validate_epoch_custom(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            current_batch = total // labels.size(0)\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss/current_batch:.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Custom CNN Training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_epoch_custom(custom_model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss, val_acc = validate_epoch_custom(custom_model, val_loader, criterion, device)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save history\n",
    "    history_custom['train_loss'].append(train_loss)\n",
    "    history_custom['train_acc'].append(train_acc)\n",
    "    history_custom['val_loss'].append(val_loss)\n",
    "    history_custom['val_acc'].append(val_acc)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(custom_model.state_dict())\n",
    "        patience_counter = 0\n",
    "        print(f\"  ✓ New best validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n⚠ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Load best model weights\n",
    "custom_model.load_state_dict(best_model_wts)\n",
    "print(f\"\\n✓ Training completed!\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Total training time: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# Save the trained model\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "model_path = models_dir / \"custom_cnn_medical_mnist.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': custom_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'history': history_custom,\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': class_names\n",
    "}, str(model_path))\n",
    "\n",
    "print(f\"✓ Model saved to: {model_path.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3edba67",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)\n",
    "\n",
    "The **Vision Transformer (ViT)** represents a paradigm shift in computer vision, adapting the Transformer architecture (originally designed for natural language processing) to image classification tasks. Unlike CNNs that use convolutional operations, ViTs process images as sequences of patches.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Image Patching**: \n",
    "   - Input images are divided into fixed-size patches (e.g., 16×16 or 32×32 pixels)\n",
    "   - Each patch is flattened and linearly projected into an embedding space\n",
    "   - This converts the 2D image into a 1D sequence of patch embeddings\n",
    "\n",
    "2. **Position Embeddings**:\n",
    "   - Learnable position embeddings are added to patch embeddings\n",
    "   - Allows the model to understand spatial relationships between patches\n",
    "\n",
    "3. **Transformer Encoder**:\n",
    "   - Multiple layers of self-attention and feed-forward networks\n",
    "   - Self-attention mechanism enables the model to focus on relevant patches\n",
    "   - Each layer refines the patch representations\n",
    "\n",
    "4. **Classification Token**:\n",
    "   - A special [CLS] token is prepended to the sequence\n",
    "   - This token aggregates information from all patches\n",
    "   - Final classification is performed using this token\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Global Receptive Field**: Self-attention allows the model to attend to all patches simultaneously, capturing long-range dependencies\n",
    "- **Scalability**: Performance improves significantly with more data and larger models\n",
    "- **Transfer Learning**: Pre-trained ViTs can be fine-tuned effectively on downstream tasks\n",
    "- **Interpretability**: Attention maps can visualize which image regions the model focuses on\n",
    "\n",
    "### Comparison with CNNs\n",
    "\n",
    "- **CNNs**: Inductive bias of locality and translation equivariance (convolutional operations)\n",
    "- **ViTs**: Minimal inductive bias, rely on attention mechanisms and data to learn patterns\n",
    "- **Data Requirements**: ViTs typically require more data than CNNs to achieve similar performance, but excel with large-scale pre-training\n",
    "\n",
    "For this project, we will train a Vision Transformer to compare its performance against the CNN-based architectures (ResNet-50 and Custom CNN) on the Medical MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4104c5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:30:06.312571Z",
     "start_time": "2025-11-17T11:30:02.460783Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "from transformers import DefaultDataCollator\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25ea7db8a2e76903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:30:07.668866Z",
     "start_time": "2025-11-17T11:30:07.666338Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Custom Dataset Class for Hugging Face\n",
    "# ============================================================================\n",
    "class MedicalMNISTDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, processor):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Process image with ViT processor\n",
    "        encoding = self.processor(image, return_tensors=\"pt\")\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d94ab48201511d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:30:36.784184Z",
     "start_time": "2025-11-17T11:30:12.611823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vision Transformer processor and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ViT model loaded and moved to mps\n",
      "  Total parameters: 85,803,270\n",
      "  Trainable parameters: 85,803,270\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Prepare Dataset\n",
    "# ============================================================================\n",
    "print(\"Loading Vision Transformer processor and model...\")\n",
    "\n",
    "# Load ViT processor and model\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"✓ ViT model loaded and moved to {device}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c07fc7e5ddb134fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:34:57.212392Z",
     "start_time": "2025-11-17T11:34:57.208839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare image paths and labels for train/val sets\n",
    "def prepare_dataset_paths(dataset_folder, class_names):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = dataset_folder / class_name\n",
    "        if class_dir.exists():\n",
    "            for img_file in class_dir.glob('*'):\n",
    "                if img_file.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                    image_paths.append(str(img_file))\n",
    "                    labels.append(class_idx)\n",
    "\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e877ed69b153f5f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:34:58.863308Z",
     "start_time": "2025-11-17T11:34:58.767871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing datasets...\n",
      "  Randomly sampled 300 training images from 47163 total\n",
      "  Randomly sampled 50 validation images from 5895 total\n",
      "  Training images: 300\n",
      "  Validation images: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPreparing datasets...\")\n",
    "train_paths, train_labels = prepare_dataset_paths(train_dir, class_names)\n",
    "val_paths, val_labels = prepare_dataset_paths(val_dir, class_names)\n",
    "\n",
    "# Randomly sample 300 images for training and 50 for validation (for faster training)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Sample training data\n",
    "if len(train_paths) > 300:\n",
    "    indices = list(range(len(train_paths)))\n",
    "    sampled_indices = random.sample(indices, 300)\n",
    "    train_paths = [train_paths[i] for i in sampled_indices]\n",
    "    train_labels = [train_labels[i] for i in sampled_indices]\n",
    "    print(f\"  Randomly sampled 300 training images from {len(indices)} total\")\n",
    "\n",
    "# Sample validation data\n",
    "if len(val_paths) > 50:\n",
    "    indices = list(range(len(val_paths)))\n",
    "    sampled_indices = random.sample(indices, 50)\n",
    "    val_paths = [val_paths[i] for i in sampled_indices]\n",
    "    val_labels = [val_labels[i] for i in sampled_indices]\n",
    "    print(f\"  Randomly sampled 50 validation images from {len(indices)} total\")\n",
    "\n",
    "print(f\"  Training images: {len(train_paths)}\")\n",
    "print(f\"  Validation images: {len(val_paths)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_hf = MedicalMNISTDataset(train_paths, train_labels, processor)\n",
    "val_dataset_hf = MedicalMNISTDataset(val_paths, val_labels, processor)\n",
    "\n",
    "# ============================================================================\n",
    "# Training Configuration\n",
    "# ============================================================================\n",
    "output_dir = \"./models/vit_medical_mnist\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{output_dir}/logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "204eff8a3f44d730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:35:01.366342Z",
     "start_time": "2025-11-17T11:35:01.357144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize Trainer\n",
    "# ============================================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_hf,\n",
    "    eval_dataset=val_dataset_hf,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "957b08d7faa29f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:38:40.474806Z",
     "start_time": "2025-11-17T11:35:02.879318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Vision Transformer Training...\n",
      "============================================================\n",
      "Training on 300 samples\n",
      "Validating on 50 samples\n",
      "Output directory: ./models/vit_medical_mnist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [380/380 03:36, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training completed!\n",
      "  Final training loss: 0.0004\n",
      "  Model saved to: ./models/vit_medical_mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sztaki/Documents/computer_vision/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "  Validation Loss: 0.0004\n",
      "  Validation Accuracy: 1.0000\n",
      "\n",
      "✓ Vision Transformer training pipeline completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Training\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Vision Transformer Training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training on {len(train_dataset_hf)} samples\")\n",
    "print(f\"Validating on {len(val_dataset_hf)} samples\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model()\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\n✓ Training completed!\")\n",
    "print(f\"  Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Model saved to: {output_dir}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"  Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Vision Transformer training pipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adbb541b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T11:40:04.916734Z",
     "start_time": "2025-11-17T11:40:02.599825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100/100 = 100%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Test: Random 100 images classification with ViT\n",
    "# ============================================================================\n",
    "# Load trained model\n",
    "output_dir = \"./models/vit_medical_mnist\"\n",
    "processor = ViTImageProcessor.from_pretrained(output_dir)\n",
    "model = ViTForImageClassification.from_pretrained(output_dir)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get 100 random test images\n",
    "test_paths, test_labels = prepare_dataset_paths(test_dir, class_names)\n",
    "random.seed(42)\n",
    "if len(test_paths) > 100:\n",
    "    sampled = random.sample(list(range(len(test_paths))), 100)\n",
    "    test_paths = [test_paths[i] for i in sampled]\n",
    "    test_labels = [test_labels[i] for i in sampled]\n",
    "\n",
    "# Classify\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for img_path, true_label in zip(test_paths, test_labels):\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        pred = outputs.logits.argmax(-1).item()\n",
    "        if pred == true_label:\n",
    "            correct += 1\n",
    "\n",
    "print(f\"Test Accuracy: {correct}/100 = {correct}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
